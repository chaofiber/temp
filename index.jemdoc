# jemdoc: menu{MENU}{index.html}
= Chao Ni 

~~~
{}{img_left}{nichao.jpg}{photo}{150px}{150px}{}
Chao Ni \n
Master Student in Robotics at ETH Zürich, [doc/Resume_Jan21.pdf CV] \n
~~~
\n
:{} /"In me, the tiger sniffs the rose." —— Siegfried Sassoon/

== Contact
- [chao.ni@inf.ethz.ch]
- Buchwiesen 2, 8052 Zürich
- +41 788605204
- [https://github.com/chaofiber Github]

== Research Interests
I am particularly fascinated with research at the intersection of robotics and algorithmic and theoretical techniques (e.g., nonlinear control methods, machine learning, optimization, etc.). I am also interested in the cutting-edge reinforcement learning approaches in the control area.

== Education
- /MSc/ in Robotics, ETH Zürich, Zürich, Switzerland, 2019-
- /BSc/ in Theoretical and Applied Mechanics, Peking University, Beijing, China, 2015-2019
- /BSc/ in Economics, Peking University, Beijing, China, 2016-2019
- Visiting student, LCSR lab, Johns Hopkins University, Baltimore, USA, 2018.6-2018.9 advisor:
 [https://me.jhu.edu/faculty/gregory-s-chirikjian/ Gregory Chirikjian]
- Research assistant, Machine Intelligence Group, Beijing, China, 2019.1-2019.8 advisor: 
 [http://iiis.tsinghua.edu.cn/~zhang/ Chongjie Zhang]

== Projects

== MPC-feedback Trajectory Optimization for Wheeled-legged Robots
~~~
{}{img_left}{doc/traj.png}{photo}{360px}{220px}{}

Wheeled-legged robots can cope with tough terrains in an energy-efficient way with the help of wheels, while it also preserves its ability to negotiate complex terrains through the presence of the leg.  To achieve various tasks in different terrains, trajectory optimization (TO) is required to serve as guidance.  Furthermore, a model predictive control (MPC) feedback planner is utilized to track this reference trajectory. In this thesis, we model the robot as a single rigid body, and the customized TOWR is used to handle the wheeled-legged robot trajectory optimization problem.  The tracking reference is converted into joint space via inverse kinematics and be fed into MPC as guidance.  We achieve this by adding the trajectory to the cost term of  the  MPC.  A  whole-body  controller  is  used  to  generate  torque  commands  for the real robots.  The framework is verified on simulation and hardware.  We show that such a framework can be modularized and TOWR can be replaced by other optimizers.   MPC  can  help  correct  the  infeasibility  of  the  trajectory  to  make  it physically practicable, as well as smooth the trajectory to avoid wild motions.  Such a trajectory-correction scheme can be further explored to realize online trajectory computing and tracking. \n 
[doc/semester_thesis.pdf thesis]  [doc/semester_thesis_ppt.pdf slide] \n
~~~

== Hexapod Robot Control
~~~
{}{img_left}{doc/UI.png}{photo}{360px}{220px}{}
In this project, we developed an inverse kinematic solver for the hexapod and generated different gaits and propose a transition state when the hexapod has to change their gaits. We also implemented obstable avoidance algorithms on the tough terrains. The project and the video can be found at [https://github.com/chaofiber/hexapod/ github]\n

~~~

== Exploiting Effective Representation via Cooperative Learning of Multi-Sensory Robotics Data
~~~
{}{img_left}{doc/RL.png}{photo}{360px}{220px}{}
Extracting effective representations from high-dimensional data has been one of the
core problems for deep learning. Since the collected data by a reinforcement learning
agent are unlabeled, previous reinforcement learning methods learn task-dependent
representations in the policy learning process or task-agnostic representations by
predicting the future in the pre-training. In robotics tasks, multi-sensory data (e.g.,
robotic control state and camera observation) are usually accessible and provide
abundant multi-source information. In this paper, we present a novel cooperative
learning framework to extract effective representations from multi-sensory data. Our
framework integrates self-supervised predictive dynamics learning and contrastive
synchronization learning, which respectively leverages the dynamics information of
each data source and the natural temporal synchronization of multi-source data. We
evaluate our method on popular robotic control tasks, and results show that our model
learns effective generic representations for down-stream reinforcement learning tasks
and enables efficient transfer among multi-sensor data.\n

Collaborating with Guangxiang Zhu*, Hao Cheng, advised by Chongjie Zhang.\n
[doc/undergrad_thesis.pdf undergraduate thesis]
~~~

== Markov Decision Process and Variants: a Survey on Robot Learning
~~~
{}{img_left}{doc/pomdp.png}{photo}{360px}{220px}{}
In this survey, I want to summarize approaches in robot reinforcement learning. The survey mainly introduced Markov Decision Process based methods including some variants including partially observable Markov Decision Process, Bayesian learning based POMDP, model predictive control approach and KL divergence upper bound based POMDP. Apart from applying other cutting-edge machine learning and deep learning methods, we can also make use of other control methods such model predictive control in robot learning problems. [doc/survey_rl.pdf pdf]\n
~~~

== GORA-Net: a Temporal Reparameterization Method for Recognizing Actions in Video Sequences
~~~
{}{img_left}{doc/gora_c3d.png}{photo}{360px}{220px}{}
In this project, our object is video of robots which are making some motions. We need to find out what motion the video actually represents. Interestingly, one essentially same motion may be carried by two videos with different temporal distribution. We try to firstly quotient out the temporal fluctuations of the video, which means to make the sequence play speed at a standard time scale. After that, we build a deep learning framework to learn the motion represented by the video. The temporal reparameterization mapping can be regarded as a hidden layer in the recurrent neural network. [doc/action-recognition-project.pdf pdf] [doc/GORA-Net-Presentation.pdf slide]\n

Collaborating with Sipu Ruan, advised by Gregory Chirikjian.
~~~

#== Model-free Methods in Policy Search: a Talk

#This talk is presented on a seminar at the RL group of Tsinghua University directed by [http://iiis.tsinghua.edu.cn/~zhang/ Chongjie #Zhang]. In this talk, basic idea of reinforcement learning as well as some model-free methods are introduced. Two popular direction of #reinforcement learning include policy gradient and weighted maximum likelihood approaches. In addition, model-based policy search #method like PILCO is also shown in the talk. [doc/talk_rl.pptx slides]\n

== Notes
Installation of HSL: [ma57.html]\n
